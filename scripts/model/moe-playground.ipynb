{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from internvl2 import InternVLChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"OpenGVLab/InternVL2-2B\"\n",
    "model = InternVLChatModel.from_pretrained(name, torch_dtype=torch.bfloat16, device_map='cuda:7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<|im_end|><|im_start|>assistant\\n'\n",
    "tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_video\n",
    "video_path = 'localdata/red-panda.mp4'\n",
    "pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).to(model.device)\n",
    "video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "question = video_prefix + 'What is the red panda doing?'\n",
    "# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = pixel_values.repeat(2, 1, 1, 1)\n",
    "print(input_ids.shape, attention_mask.shape, pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_patches_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch_chat(tokenizer, pixel_values, ['What is the red panda doing?', 'What is the red panda eating?'], generation_config, num_patches_list=[8, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MoE Video Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moe_reward import InternVLChatRewardModeling, InternVLChatRewardModelingConfig\n",
    "from transformers import AutoTokenizer\n",
    "from internvl2 import InternVLChatModel, InternVLChatConfig, prepare_chat_input\n",
    "import torch\n",
    "\n",
    "from torch import distributed as dist\n",
    "import os\n",
    "\n",
    "os.environ['WORLD_SIZE'] = str(1)\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = str(12345)\n",
    "os.environ['LOCAL_RANK'] = str(0)\n",
    "os.environ['RANK'] = str(0)\n",
    "\n",
    "\n",
    "dist.init_process_group(backend='nccl', world_size=1, rank=0)\n",
    "\n",
    "name = \"OpenGVLab/InternVL2-2B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, trust_remote_code=True)\n",
    "config = InternVLChatRewardModelingConfig.from_pretrained(name, pad_token_id=tokenizer.pad_token_id, num_objectives=10, num_aspects=3, aspect2criteria={\n",
    "    0: [0, 1, 2],\n",
    "    1: [3, 4, 5],\n",
    "    2: [6, 7, 8, 9]\n",
    "}, gating_temperature=1.0, gating_hidden_dim=1024, gating_n_hidden=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InternVLChatRewardModeling(name=name, config=config)\n",
    "model = model.to(torch.bfloat16).to('cuda:7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CONTEXT_TOKEN='<IMG_CONTEXT>'\n",
    "model.model.img_context_token_id = tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)\n",
    "print(model.model.img_context_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_video\n",
    "import torch\n",
    "\n",
    "def pad_to_batch(pad_token_id, input_ids_list: list, attention_mask_list: list, pixel_values_list: list):\n",
    "    max_len = max(input_ids.shape[-1] for input_ids in input_ids_list)\n",
    "    for i in range(len(input_ids_list)):\n",
    "        input_ids_list[i] = torch.cat(\n",
    "            [input_ids_list[i], torch.full((input_ids_list[i].shape[0], max_len - input_ids_list[i].shape[-1]), pad_token_id, dtype=input_ids_list[i].dtype, device=input_ids_list[i].device)], dim=-1\n",
    "        )\n",
    "        attention_mask_list[i] = torch.cat(\n",
    "            [attention_mask_list[i], torch.zeros((attention_mask_list[i].shape[0], max_len - attention_mask_list[i].shape[-1]), dtype=attention_mask_list[i].dtype, device=attention_mask_list[i].device)], dim=-1\n",
    "        )\n",
    "    \n",
    "    input_ids_list = torch.cat(input_ids_list, dim=0)\n",
    "    attention_mask_list = torch.cat(attention_mask_list, dim=0)\n",
    "    pixel_values_list = torch.cat(pixel_values_list, dim=0)\n",
    "\n",
    "    return input_ids_list, attention_mask_list, pixel_values_list\n",
    "    \n",
    "\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "video_path = 'localdata/red-panda.mp4'\n",
    "pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).to(model.model.device)\n",
    "video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "question1 = video_prefix + 'What is the red panda doing?'\n",
    "question2 = video_prefix + 'What is the red panda eating? and what is the red panda doing? and how many red pandas are there?'\n",
    "\n",
    "input_ids1, attention_mask1 = prepare_chat_input(config, tokenizer, pixel_values, question1, generation_config, device=model.model.device)\n",
    "input_ids2, attention_mask2 = prepare_chat_input(config, tokenizer, pixel_values, question2, generation_config, device=model.model.device)\n",
    "\n",
    "\n",
    "input_ids, attention_mask, pixel_values = pad_to_batch(tokenizer.pad_token_id, [input_ids1, input_ids2], [attention_mask1, attention_mask2], [pixel_values, pixel_values])\n",
    "print(input_ids1.shape, input_ids2.shape)\n",
    "print(input_ids.shape, attention_mask.shape, pixel_values.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(input_ids[:, -19:], attention_mask[:, :-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.forward(pixel_values, input_ids, attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
